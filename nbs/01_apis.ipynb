{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/BigStorage/MerlinsPlace/mmm/merlins-mirror/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from controlnet_aux import MidasDetector, OpenposeDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_model = MidasDetector.from_pretrained(\"lllyasviel/Annotators\")\n",
    "pose_model  = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m  \n",
      "pose_model(\n",
      "    input_image,\n",
      "    detect_resolution=\u001b[32m512\u001b[39m,\n",
      "    image_resolution=\u001b[32m512\u001b[39m,\n",
      "    include_body=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    include_hand=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    include_face=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    hand_and_face=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    output_type=\u001b[33m'pil'\u001b[39m,\n",
      "    **kwargs,\n",
      ")\n",
      "\u001b[31mType:\u001b[39m        OpenposeDetector\n",
      "\u001b[31mString form:\u001b[39m <controlnet_aux.open_pose.OpenposeDetector object at 0x7a97c0184ec0>\n",
      "\u001b[31mFile:\u001b[39m        /BigStorage/MerlinsPlace/mmm/merlins-mirror/.venv/lib/python3.12/site-packages/controlnet_aux/open_pose/__init__.py\n",
      "\u001b[31mSource:\u001b[39m     \n",
      "\u001b[38;5;28;01mclass\u001b[39;00m OpenposeDetector:\n",
      "    \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[33m    A class for detecting human poses in images using the Openpose model.\u001b[39m\n",
      "\n",
      "\u001b[33m    Attributes:\u001b[39m\n",
      "\u001b[33m        model_dir (str): Path to the directory where the pose models are stored.\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __init__(self, body_estimation, hand_estimation=\u001b[38;5;28;01mNone\u001b[39;00m, face_estimation=\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "        self.body_estimation = body_estimation\n",
      "        self.hand_estimation = hand_estimation\n",
      "        self.face_estimation = face_estimation\n",
      "\n",
      "    @classmethod\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m from_pretrained(cls, pretrained_model_or_path, filename=\u001b[38;5;28;01mNone\u001b[39;00m, hand_filename=\u001b[38;5;28;01mNone\u001b[39;00m, face_filename=\u001b[38;5;28;01mNone\u001b[39;00m, cache_dir=\u001b[38;5;28;01mNone\u001b[39;00m, local_files_only=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_or_path == \u001b[33m\"lllyasviel/ControlNet\"\u001b[39m:\n",
      "            filename = filename \u001b[38;5;28;01mor\u001b[39;00m \u001b[33m\"annotator/ckpts/body_pose_model.pth\"\u001b[39m\n",
      "            hand_filename = hand_filename \u001b[38;5;28;01mor\u001b[39;00m \u001b[33m\"annotator/ckpts/hand_pose_model.pth\"\u001b[39m\n",
      "            face_filename = face_filename \u001b[38;5;28;01mor\u001b[39;00m \u001b[33m\"facenet.pth\"\u001b[39m\n",
      "\n",
      "            face_pretrained_model_or_path = \u001b[33m\"lllyasviel/Annotators\"\u001b[39m\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            filename = filename \u001b[38;5;28;01mor\u001b[39;00m \u001b[33m\"body_pose_model.pth\"\u001b[39m\n",
      "            hand_filename = hand_filename \u001b[38;5;28;01mor\u001b[39;00m \u001b[33m\"hand_pose_model.pth\"\u001b[39m\n",
      "            face_filename = face_filename \u001b[38;5;28;01mor\u001b[39;00m \u001b[33m\"facenet.pth\"\u001b[39m\n",
      "\n",
      "            face_pretrained_model_or_path = pretrained_model_or_path\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(pretrained_model_or_path):\n",
      "            body_model_path = os.path.join(pretrained_model_or_path, filename)\n",
      "            hand_model_path = os.path.join(pretrained_model_or_path, hand_filename)\n",
      "            face_model_path = os.path.join(face_pretrained_model_or_path, face_filename)\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            body_model_path = hf_hub_download(pretrained_model_or_path, filename, cache_dir=cache_dir, local_files_only=local_files_only)\n",
      "            hand_model_path = hf_hub_download(pretrained_model_or_path, hand_filename, cache_dir=cache_dir, local_files_only=local_files_only)\n",
      "            face_model_path = hf_hub_download(face_pretrained_model_or_path, face_filename, cache_dir=cache_dir, local_files_only=local_files_only)\n",
      "\n",
      "        body_estimation = Body(body_model_path)\n",
      "        hand_estimation = Hand(hand_model_path)\n",
      "        face_estimation = Face(face_model_path)\n",
      "\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m cls(body_estimation, hand_estimation, face_estimation)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m to(self, device):\n",
      "        self.body_estimation.to(device)\n",
      "        self.hand_estimation.to(device)\n",
      "        self.face_estimation.to(device)\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m self\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m detect_hands(self, body: BodyResult, oriImg) -> Tuple[Union[HandResult, \u001b[38;5;28;01mNone\u001b[39;00m], Union[HandResult, \u001b[38;5;28;01mNone\u001b[39;00m]]:\n",
      "        left_hand = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        right_hand = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        H, W, _ = oriImg.shape\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m x, y, w, is_left \u001b[38;5;28;01min\u001b[39;00m util.handDetect(body, oriImg):\n",
      "            peaks = self.hand_estimation(oriImg[y:y+w, x:x+w, :]).astype(np.float32)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m peaks.ndim == \u001b[32m2\u001b[39m \u001b[38;5;28;01mand\u001b[39;00m peaks.shape[\u001b[32m1\u001b[39m] == \u001b[32m2\u001b[39m:\n",
      "                peaks[:, \u001b[32m0\u001b[39m] = np.where(peaks[:, \u001b[32m0\u001b[39m] < \u001b[32m1e-6\u001b[39m, -\u001b[32m1\u001b[39m, peaks[:, \u001b[32m0\u001b[39m] + x) / float(W)\n",
      "                peaks[:, \u001b[32m1\u001b[39m] = np.where(peaks[:, \u001b[32m1\u001b[39m] < \u001b[32m1e-6\u001b[39m, -\u001b[32m1\u001b[39m, peaks[:, \u001b[32m1\u001b[39m] + y) / float(H)\n",
      "                \n",
      "                hand_result = [\n",
      "                    Keypoint(x=peak[\u001b[32m0\u001b[39m], y=peak[\u001b[32m1\u001b[39m])\n",
      "                    \u001b[38;5;28;01mfor\u001b[39;00m peak \u001b[38;5;28;01min\u001b[39;00m peaks\n",
      "                ]\n",
      "\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m is_left:\n",
      "                    left_hand = hand_result\n",
      "                \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                    right_hand = hand_result\n",
      "\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m left_hand, right_hand\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m detect_face(self, body: BodyResult, oriImg) -> Union[FaceResult, \u001b[38;5;28;01mNone\u001b[39;00m]:\n",
      "        face = util.faceDetect(body, oriImg)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m face \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        \n",
      "        x, y, w = face\n",
      "        H, W, _ = oriImg.shape\n",
      "        heatmaps = self.face_estimation(oriImg[y:y+w, x:x+w, :])\n",
      "        peaks = self.face_estimation.compute_peaks_from_heatmaps(heatmaps).astype(np.float32)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m peaks.ndim == \u001b[32m2\u001b[39m \u001b[38;5;28;01mand\u001b[39;00m peaks.shape[\u001b[32m1\u001b[39m] == \u001b[32m2\u001b[39m:\n",
      "            peaks[:, \u001b[32m0\u001b[39m] = np.where(peaks[:, \u001b[32m0\u001b[39m] < \u001b[32m1e-6\u001b[39m, -\u001b[32m1\u001b[39m, peaks[:, \u001b[32m0\u001b[39m] + x) / float(W)\n",
      "            peaks[:, \u001b[32m1\u001b[39m] = np.where(peaks[:, \u001b[32m1\u001b[39m] < \u001b[32m1e-6\u001b[39m, -\u001b[32m1\u001b[39m, peaks[:, \u001b[32m1\u001b[39m] + y) / float(H)\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m [\n",
      "                Keypoint(x=peak[\u001b[32m0\u001b[39m], y=peak[\u001b[32m1\u001b[39m])\n",
      "                \u001b[38;5;28;01mfor\u001b[39;00m peak \u001b[38;5;28;01min\u001b[39;00m peaks\n",
      "            ]\n",
      "        \n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m detect_poses(self, oriImg, include_hand=\u001b[38;5;28;01mFalse\u001b[39;00m, include_face=\u001b[38;5;28;01mFalse\u001b[39;00m) -> List[PoseResult]:\n",
      "        \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[33m        Detect poses in the given image.\u001b[39m\n",
      "\u001b[33m            Args:\u001b[39m\n",
      "\u001b[33m                oriImg (numpy.ndarray): The input image for pose detection.\u001b[39m\n",
      "\u001b[33m                include_hand (bool, optional): Whether to include hand detection. Defaults to False.\u001b[39m\n",
      "\u001b[33m                include_face (bool, optional): Whether to include face detection. Defaults to False.\u001b[39m\n",
      "\n",
      "\u001b[33m        Returns:\u001b[39m\n",
      "\u001b[33m            List[PoseResult]: A list of PoseResult objects containing the detected poses.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        oriImg = oriImg[:, :, ::-\u001b[32m1\u001b[39m].copy()\n",
      "        H, W, C = oriImg.shape\n",
      "        \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "            candidate, subset = self.body_estimation(oriImg)\n",
      "            bodies = self.body_estimation.format_body_result(candidate, subset)\n",
      "\n",
      "            results = []\n",
      "            \u001b[38;5;28;01mfor\u001b[39;00m body \u001b[38;5;28;01min\u001b[39;00m bodies:\n",
      "                left_hand, right_hand, face = (\u001b[38;5;28;01mNone\u001b[39;00m,) * \u001b[32m3\u001b[39m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m include_hand:\n",
      "                    left_hand, right_hand = self.detect_hands(body, oriImg)\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m include_face:\n",
      "                    face = self.detect_face(body, oriImg)\n",
      "                \n",
      "                results.append(PoseResult(BodyResult(\n",
      "                    keypoints=[\n",
      "                        Keypoint(\n",
      "                            x=keypoint.x / float(W),\n",
      "                            y=keypoint.y / float(H)\n",
      "                        ) \u001b[38;5;28;01mif\u001b[39;00m keypoint \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "                        \u001b[38;5;28;01mfor\u001b[39;00m keypoint \u001b[38;5;28;01min\u001b[39;00m body.keypoints\n",
      "                    ], \n",
      "                    total_score=body.total_score,\n",
      "                    total_parts=body.total_parts\n",
      "                ), left_hand, right_hand, face))\n",
      "            \n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "        \n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __call__(self, input_image, detect_resolution=\u001b[32m512\u001b[39m, image_resolution=\u001b[32m512\u001b[39m, include_body=\u001b[38;5;28;01mTrue\u001b[39;00m, include_hand=\u001b[38;5;28;01mFalse\u001b[39;00m, include_face=\u001b[38;5;28;01mFalse\u001b[39;00m, hand_and_face=\u001b[38;5;28;01mNone\u001b[39;00m, output_type=\u001b[33m\"pil\"\u001b[39m, **kwargs):\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m hand_and_face \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            warnings.warn(\u001b[33m\"hand_and_face is deprecated. Use include_hand and include_face instead.\"\u001b[39m, DeprecationWarning)\n",
      "            include_hand = hand_and_face\n",
      "            include_face = hand_and_face\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"return_pil\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m kwargs:\n",
      "            warnings.warn(\u001b[33m\"return_pil is deprecated. Use output_type instead.\"\u001b[39m, DeprecationWarning)\n",
      "            output_type = \u001b[33m\"pil\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs[\u001b[33m\"return_pil\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"np\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m type(output_type) \u001b[38;5;28;01mis\u001b[39;00m bool:\n",
      "            warnings.warn(\u001b[33m\"Passing `True` or `False` to `output_type` is deprecated and will raise an error in future versions\"\u001b[39m)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m output_type:\n",
      "                output_type = \u001b[33m\"pil\"\u001b[39m\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(input_image, np.ndarray):\n",
      "            input_image = np.array(input_image, dtype=np.uint8)\n",
      "\n",
      "        input_image = HWC3(input_image)\n",
      "        input_image = resize_image(input_image, detect_resolution)\n",
      "        H, W, C = input_image.shape\n",
      "        \n",
      "        poses = self.detect_poses(input_image, include_hand, include_face)\n",
      "        canvas = draw_poses(poses, H, W, draw_body=include_body, draw_hand=include_hand, draw_face=include_face) \n",
      "\n",
      "        detected_map = canvas\n",
      "        detected_map = HWC3(detected_map)\n",
      "        \n",
      "        img = resize_image(input_image, image_resolution)\n",
      "        H, W, C = img.shape\n",
      "\n",
      "        detected_map = cv2.resize(detected_map, (W, H), interpolation=cv2.INTER_LINEAR)\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m output_type == \u001b[33m\"pil\"\u001b[39m:\n",
      "            detected_map = Image.fromarray(detected_map)\n",
      "\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m detected_map"
     ]
    }
   ],
   "source": [
    "pose_model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models import controlnets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'diffusers.models.controlnets.controlnet_union' from '/BigStorage/MerlinsPlace/mmm/merlins-mirror/.venv/lib/python3.12/site-packages/diffusers/models/controlnets/controlnet_union.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controlnets.controlnet_union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ControlNetUnionInputProMax??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import error: No module named 'triton'\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'apply_dynamic_quant' from 'torchao.quantization' (/Users/cck/projects/mirror-ai/.venv/lib/python3.12/site-packages/torchao/quantization/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchao\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_dynamic_quant\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'apply_dynamic_quant' from 'torchao.quantization' (/Users/cck/projects/mirror-ai/.venv/lib/python3.12/site-packages/torchao/quantization/__init__.py)"
     ]
    }
   ],
   "source": [
    "from torchao.quantization import apply_dynamic_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m sse_message(elm, event=\u001b[33m'message'\u001b[39m)\n",
      "\u001b[31mSource:\u001b[39m   \n",
      "\u001b[38;5;28;01mdef\u001b[39;00m sse_message(elm, event=\u001b[33m'message'\u001b[39m):\n",
      "    \u001b[33m\"Convert element `elm` into a format suitable for SSE streaming\"\u001b[39m\n",
      "    data = \u001b[33m'\\n'\u001b[39m.join(f'data: {o}' \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;28;01min\u001b[39;00m to_xml(elm).splitlines())\n",
      "    \u001b[38;5;28;01mreturn\u001b[39;00m f'event: {event}\\n{data}\\n\\n'\n",
      "\u001b[31mFile:\u001b[39m      ~/projects/mirror-ai/.venv/lib/python3.12/site-packages/fasthtml/components.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "sse_message??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdown_event = signal_shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<asyncio.locks.Event object at 0x10ce9d340 [unset]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutdown_event"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
